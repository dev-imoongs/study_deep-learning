#!/usr/bin/env python
# coding: utf-8

# ### ImageDataGenerator Task
# 
# ##### ë™ë¬¼ ë¶„ë¥˜
# https://drive.google.com/file/d/1_d8RcCM21XneorFe_m4939erMkCnccS1/view?usp=drive_link
# 
# ### Preprocessing, Data Loading
# - Preprocessingê³¼ Data Loadingì€ ImageDataGeneratorê°ì²´ì™€ modelì˜ fit_generator()ê°€ ì—°ê²°ë˜ì–´ ìˆ˜í–‰ëœë‹¤.
# - modelì˜ fit_generator()ê°€ ImageDataGenerator ê°ì²´ë¥¼ ì „ë‹¬ ë°›ì€ ë’¤ Pipeline Streamìœ¼ë¡œ ì´ì–´ì§€ê²Œ êµ¬ì„±ëœë‹¤.
# 1. ì´ë¯¸ì§€ íŒŒì¼ì„ ì´ë¯¸ì§€ ë°°ì—´ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
# 2. ì „ì²˜ë¦¬(preprocessing) ì ìš©
# > - Augmentation
# > - ì´ë¯¸ì§€ ë°°ì—´ ê°’ scale ì¡°ì • (0 ~ 1,  float32ë¡œ í˜•ë³€í™˜)
# > - ì´ë¯¸ì§€ ë°°ì—´ í¬ê¸° ì¬ì¡°ì •
# > - Normalization(í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ ì¬ì¡°ì •)
# > - ì´ì§„ ë¶„ë¥˜, ë‹¤ì¤‘ ë¶„ë¥˜ì— ë”°ë¼ Encoding ì§„í–‰
# > - ë‹¤ì¤‘ ë¶„ë¥˜ì¼ ê²½ìš° One-hot encoding, Lavbel Encoding ì¤‘ í•œ ê°€ì§€ ì§„í–‰
# 3. ì´ë¯¸ì§€ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ì´ë¯¸ì§€ ë°°ì—´ í˜•íƒœë¡œ ë¡œë”©
# 4. ì´ë¯¸ì§€ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë”©í•  ë•Œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¼ì • í¬ê¸° ë‹¨ìœ„(batch)ë¡œ ì‘ì—… ì§„í–‰
# 5. fit_generator() í˜¸ì¶œ ì‹œ, ì´ì „ CPU ì‘ì—…ì´ GPUë¡œ ì´ë™ í›„ ì‘ì—… ì§„í–‰
# - ì‹¤ì œ Preprocessingê³¼ Data Loadingì€ Modelì„ í†µí•´ fit_generator()ë¥¼ í˜¸ì¶œí•˜ê¸° ì „ê¹Œì§€ëŠ” ìˆ˜í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤(ëŒ€ê¸° ìƒíƒœ).  
# 
# ğŸ“Œ CPUëŠ” ì»´í“¨í„°ì˜ ë‡Œë¡œì„œ, ì—°ì‚°ì´ ì–´ë ¤ìš´ ì¼ë ¨ì˜ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ì‘ì—…, ì»´í“¨í„°ê°€ ëŒì•„ê°€ëŠ” ë°ì— ìˆì–´ì„œ ì¤‘ìš”í•œ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ë°ì— ì–´ìš¸ë¦¬ê³ , GPUëŠ” ì–‘ì´ êµ‰ì¥íˆ ë§ì€ ë‹¨ìˆœí•œ ì—°ì‚°(ë‚´ì , ë²¡í„° ë“±)ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆëŠ” CPUë¥¼ ë•ëŠ” ì¥ì¹˜ì´ë‹¤. ì‹ ê²½ë§ ê³„ì¸µ ë˜ëŠ” 2D ì´ë¯¸ì§€ì™€ ê°™ì€ ëŒ€ê·œëª¨ì˜ íŠ¹ì • ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë”¥ ëŸ¬ë‹ í›ˆë ¨ ì‘ì—…ì— ì–´ìš¸ë¦°ë‹¤. í”½ì…€ë¡œ ì´ë£¨ì–´ì§„ ì˜ìƒì„ ì²˜ë¦¬í•˜ëŠ” ìš©ë„ë¡œ ì œì‘ë˜ì—ˆìœ¼ë©°, ë°˜ë³µì ì´ê³  ë¹„ìŠ·í•œ, ëŒ€ëŸ‰ì˜ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©° ì´ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì—…í•˜ê¸° ë•Œë¬¸ì— CPUì— ë¹„í•´ ì†ë„ê°€ ëŒ€ë‹¨íˆ ë¹ ë¥´ë‹¤. ê·¸ë˜í”½ ì‘ì—…ì˜ ê²½ìš° CPUê°€ GPUë¡œ ë°ì´í„°ë¥¼ ë³´ë‚´ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•œë‹¤. CPUëŠ” ìˆœì°¨ì ì¸ ì‘ì—…, GPUëŠ” ë³‘ë ¬ì ì¸ ì‘ì—…ì— íŠ¹í™”ë˜ì–´ ìˆë‹¤. CPUì™€ GPUì˜ ì¡°í•©, ê±°ê¸°ì— ì¶©ë¶„í•œ RAMì„ ë”í•˜ë©´ ë”¥ ëŸ¬ë‹ ë° AIì— ì•Œë§ì€ í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

# In[1]:


file = open('./datasets/animals/translate.py', 'r')
content = file.readline()
content = content[content.index('{'): content.index('}') + 1]
content1 = eval(content)
content2 = {v : k for k, v in content1.items()}

file.close()

print(content1, content2, sep='\n')


# In[2]:


import os
from glob import glob

root = './datasets/animals/original/'
directories = glob(os.path.join(root, '*'))
print(directories)

for directory in directories:
    try:
        os.rename(directory, os.path.join(root, content1[directory[directory.rindex('\\') + 1:]]))
    except KeyError as e:
        os.rename(directory, os.path.join(root, content2[directory[directory.rindex('\\') + 1:]]))


# In[3]:


directories = glob(os.path.join(root, '*'))
print(directories)


# In[4]:


# ë””ë ‰í† ë¦¬ ì´ë¦„ì„ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°(list íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
directory_names = list(map(lambda directory: directory[directory.rindex("\\") + 1:], directories))
directory_names


# In[5]:


# ì „ì²´ íŒŒì¼ëª…ì„ ë””ë ‰í† ë¦¬ëª…ê³¼ ì¼ì¹˜í•˜ê²Œ ë°”ê¾¸ì!(ì˜ˆ: dog1.png, dog2.png, ...)
# os.rename(old, new)
# 1. directory_names
# 2. os.listdir()
# os.listdir(os.path.join(root, 'dog'))

root = './datasets/animals/original/'

for name in directory_names:
    for i, file_name in enumerate(os.listdir(os.path.join(root, name))):
        old_file = os.path.join(root + name + '/', file_name)
        new_file = os.path.join(root + name + '/', name + str(i + 1) + '.png')
        
        os.rename(old_file, new_file)


# In[6]:


from tensorflow.keras.preprocessing.image import ImageDataGenerator

image_data_generator = ImageDataGenerator(rescale=1./255)

generator = image_data_generator.flow_from_directory(
    root,
    target_size=(150, 150),
#     CPUì— ë‚˜ëˆ ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•  ë‹¨ìœ„ ì‘ì„±(batch_size)
    batch_size = 20,
    class_mode = 'categorical'
)

print(generator.class_indices)


# In[7]:


import pandas as pd

animal_df = pd.DataFrame({'file_paths': generator.filepaths, 'targets': generator.classes})
animal_df.file_paths = animal_df.file_paths.apply(lambda x: x.replace('\\', '/'))
animal_df


# In[8]:


from sklearn.model_selection import train_test_split

train_images, test_images, train_targets, test_targets = train_test_split(animal_df.file_paths, animal_df.targets, stratify=animal_df.targets, test_size=0.2, random_state=124)

print(train_targets.value_counts())
print(test_targets.value_counts())


# In[9]:


# ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
train_images, validation_images, train_targets, validation_targets = train_test_split(train_images, train_targets, stratify=train_targets, test_size=0.2, random_state=124)

print(train_targets.value_counts())
print(validation_targets.value_counts())


# In[10]:


import shutil
import os.path

root = './datasets/animals/'


for filepath in train_images:
    animal_dir = filepath[filepath.find('original/') + 9:filepath.rindex('/')] + '/'
    destination = root + 'train/' + animal_dir
    
    if not os.path.exists(destination):
        os.mkdir(destination)
    shutil.copy2(filepath, destination)


# In[11]:


import shutil
import os.path

root = './datasets/animals/'


for filepath in validation_images:
    animal_dir = filepath[filepath.find('original/') + 9:filepath.rindex('/')] + '/'
    destination = root + 'validation/' + animal_dir
    
    if not os.path.exists(destination):
        os.mkdir(destination)
    shutil.copy2(filepath, destination)


# In[12]:


import shutil
import os.path

root = './datasets/animals/'


for filepath in test_images:
    animal_dir = filepath[filepath.find('original/') + 9:filepath.rindex('/')] + '/'
    destination = root + 'test/' + animal_dir
    
    if not os.path.exists(destination):
        os.mkdir(destination)
    shutil.copy2(filepath, destination)


# In[13]:


IMAGE_SIZE = 244
BATCH_SIZE = 20

train_dir = './datasets/animals/train/'
validation_dir = './datasets/animals/validation/'
test_dir = './datasets/animals/test'

train_generator = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, brightness_range=(0.7, 1.3), horizontal_flip=True, vertical_flip=True, rescale=1./255)
validation_generator = ImageDataGenerator(rescale=1./255)
test_generator = ImageDataGenerator(rescale=1/255.0)

train_flow = train_generator.flow_from_directory(
    train_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

validation_flow = validation_generator.flow_from_directory(
    validation_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

test_flow = test_generator.flow_from_directory(
    test_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)


# In[25]:


image_array, image_target = next(train_flow)
print(image_array.shape)
print(image_target.shape)


# In[28]:


from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Dropout, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from tensorflow.keras.regularizers import l1, l2

IMAGE_SIZE = 244

input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))

x = Conv2D(filters=64, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(input_tensor)
x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=64, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D(pool_size=2)(x)

x = Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=128, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D(pool_size=2)(x)

x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D(pool_size=2)(x)

x = Conv2D(filters=512, kernel_size=3, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)

x = GlobalAveragePooling2D()(x)
x = Dropout(rate=0.5)(x)
x = Dense(300, activation='relu', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(x)
x = Dropout(rate=0.2)(x)
output = Dense(10, activation='softmax', name='output', kernel_initializer='glorot_normal')(x)

model = Model(inputs=input_tensor, outputs=output)
model.summary()


# In[29]:


from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import Accuracy

model.compile(optimizer=Adam(0.001), loss=CategoricalCrossentropy(), metrics=['acc'])


# In[30]:


from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

mcp_cb = ModelCheckpoint(filepath='./callback_files/weights.{epoch:03d}-{val_loss:.4f}.h5', monitor='val_loss', 
                         save_best_only=True, save_weights_only=True, mode='min', verbose=1)
rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, mode='min', verbose=1)
ely_cb = EarlyStopping(monitor='val_loss', patience=4, mode='min', verbose=1)


# In[32]:


history = model.fit_generator(train_flow, epochs=10, validation_data=validation_flow, callbacks=[mcp_cb, rlr_cb, ely_cb])


# In[ ]:


model.evaluate(test_flow)


# In[ ]:


import matplotlib.pyplot as plt

plt.plot(history.history['acc'], label='train')
plt.plot(history.history['val_acc'], label='validation')
plt.legend()


# 1. ì´ë¯¸ì§€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# 2. ì´ë¯¸ì§€ ë°°ì—´ë¡œ ë³€ê²½
# 3. ì´ë¯¸ì§€ ì¦ê°•(ì„ íƒ)
# 4. flow êµ¬ì„±
# 5. ëª¨ë¸ ì œì‘
# 6. fit_generator()
# 7. evaluate
